{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O código abaixo faz parte do Tech Challenge - Fase 4 da Pós Graduação IA para Devs e se trata de um script em Python que processa um vídeo de 1 minuto previamente fornecido pela FIAP para detectar faces, reconhecer emoções, identificar movimentos corporais específicos e reconhecer indivíduos com base em imagens previamente fornecidas, neste caso apenas 3 pessoas no vídeo. Ele utiliza diversas bibliotecas de visão computacional e aprendizado de máquina, como OpenCV, face_recognition, DeepFace e MediaPipe.\n",
    "\n",
    "\n",
    "\n",
    "## Bibliotecas utlizadas no projeto:\n",
    "\n",
    "\n",
    "cv2: Biblioteca OpenCV para processamento de imagens e vídeos.\n",
    "\n",
    "face_recognition: Biblioteca para reconhecimento facial.\n",
    "\n",
    "os: Módulo para interagir com o sistema operacional.\n",
    "\n",
    "numpy: Biblioteca para manipulação de arrays e operações numéricas.\n",
    "\n",
    "tqdm: Biblioteca para criar barras de progresso.\n",
    "\n",
    "DeepFace: Framework para análise facial (emoções, idade, raça, etc.).\n",
    "\n",
    "mediapipe: Biblioteca para detecção de pose e rastreamento de corpo.\n",
    "\n",
    "## Métodos: \n",
    "\n",
    "# load_images_from_folder():\n",
    "\n",
    "Objetivo: Carregar imagens de uma pasta e extrair as codificações faciais e nomes associados.\n",
    "\n",
    "Processo:\n",
    "- Percorre todos os arquivos de imagem na pasta especificada.\n",
    "- Para cada imagem, extrai a codificação facial usando face_recognition.\n",
    "- Armazena a codificação e o nome (derivado do nome do arquivo) em listas.\n",
    "- Resultado: Retorna listas de codificações faciais conhecidas e seus nomes correspondentes.\n",
    "\n",
    "\n",
    "\n",
    "# detect_faces_and_emotions()\n",
    "\n",
    "Objetivo: Processar o vídeo para detectar faces, emoções e movimentos corporais, e salvar um vídeo de saída com as informações anotadas.\n",
    "\n",
    "Processo Geral:\n",
    "\n",
    "- Configurações Iniciais: Inicializa objetos para detecção de pose, captura de vídeo e escrita de vídeo.\n",
    "- Variáveis de Contagem: Inicializa contadores para diferentes movimentos (ex.: levantamentos de braço, pulos, sentadas, acenos, palmas).\n",
    "\n",
    "Funções Internas de Detecção:\n",
    "- is_arm_up: Verifica se o(s) braço(s) está(ão) levantado(s) acima dos olhos.\n",
    "- is_jumping: Verifica se os tornozelos estão acima de um certo nível (indicando um pulo).\n",
    "- is_sitting: Verifica a posição dos quadris em relação aos joelhos para detectar se a pessoa está sentada.\n",
    "- is_waving_hand: Verifica se o punho está acima do cotovelo, indicando um aceno.\n",
    "- is_clapping: Calcula a distância entre os punhos para detectar palmas.\n",
    "\n",
    "Processamento de Frames:\n",
    "\n",
    "- Usa um loop para percorrer cada frame do vídeo.\n",
    "\n",
    "Análise Facial:\n",
    "\n",
    "- Usa o DeepFace para analisar emoções, idade e raça em cada frame.\n",
    "- Converte o frame para RGB para uso com face_recognition.\n",
    "- Detecta locais de faces e codificações faciais no frame.\n",
    "\n",
    "Detecção de Pose:\n",
    "\n",
    "- Processa o frame com o MediaPipe para obter pontos de referência do corpo.\n",
    "- Se pontos de referência forem detectados, desenha as marcações no frame.\n",
    "- Usa as funções internas para detectar movimentos e atualiza os contadores correspondentes.\n",
    "\n",
    "Anomalias:\n",
    "\n",
    "- Se nenhum ponto de referência for detectado, incrementa o contador de anomalias.\n",
    "\n",
    "Exibição de Informações:\n",
    "\n",
    "- Escreve no frame as contagens de movimentos e informações faciais (emoção dominante, idade, raça).\n",
    "- Associa nomes às faces reconhecidas com base nas codificações conhecidas.\n",
    "\n",
    "Escrita do Frame no Vídeo de Saída:\n",
    "\n",
    "- O frame processado é escrito no arquivo de vídeo de saída.\n",
    "\n",
    "Finalização:\n",
    "\n",
    "- Libera os objetos de captura e escrita de vídeo.\n",
    "- Exibe um relatório resumido com o total de frames analisados e anomalias detectadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 220\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal de Anomalias Detectadas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manomalies_detected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    219\u001b[0m image_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/gabri/Desktop/FIAP/fiap_pos_tech/ImagesChallenge\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 220\u001b[0m known_face_encodings, known_face_names \u001b[38;5;241m=\u001b[39m \u001b[43mload_images_from_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m input_video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/gabri/Desktop/FIAP/fiap_pos_tech/VideosChallenge/videoTechChallenge.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    222\u001b[0m output_video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/gabri/Desktop/FIAP/fiap_pos_tech/VideosChallenge/output3_video_tech_challenge.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m, in \u001b[0;36mload_images_from_folder\u001b[1;34m(folder)\u001b[0m\n\u001b[0;32m     16\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, filename)\n\u001b[0;32m     17\u001b[0m image \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mload_image_file(image_path)\n\u001b[1;32m---> 18\u001b[0m face_encodings \u001b[38;5;241m=\u001b[39m \u001b[43mface_recognition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m face_encodings:\n\u001b[0;32m     20\u001b[0m     face_encoding \u001b[38;5;241m=\u001b[39m face_encodings[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\gabri\\Desktop\\FIAP\\fiap_pos_tech\\Lib\\site-packages\\face_recognition\\api.py:213\u001b[0m, in \u001b[0;36mface_encodings\u001b[1;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mface_encodings\u001b[39m(face_image, known_face_locations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, num_jitters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    Given an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    :return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     raw_landmarks \u001b[38;5;241m=\u001b[39m \u001b[43m_raw_face_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknown_face_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39marray(face_encoder\u001b[38;5;241m.\u001b[39mcompute_face_descriptor(face_image, raw_landmark_set, num_jitters)) \u001b[38;5;28;01mfor\u001b[39;00m raw_landmark_set \u001b[38;5;129;01min\u001b[39;00m raw_landmarks]\n",
      "File \u001b[1;32mc:\\Users\\gabri\\Desktop\\FIAP\\fiap_pos_tech\\Lib\\site-packages\\face_recognition\\api.py:156\u001b[0m, in \u001b[0;36m_raw_face_landmarks\u001b[1;34m(face_image, face_locations, model)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raw_face_landmarks\u001b[39m(face_image, face_locations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlarge\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m face_locations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m         face_locations \u001b[38;5;241m=\u001b[39m \u001b[43m_raw_face_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m         face_locations \u001b[38;5;241m=\u001b[39m [_css_to_rect(face_location) \u001b[38;5;28;01mfor\u001b[39;00m face_location \u001b[38;5;129;01min\u001b[39;00m face_locations]\n",
      "File \u001b[1;32mc:\\Users\\gabri\\Desktop\\FIAP\\fiap_pos_tech\\Lib\\site-packages\\face_recognition\\api.py:105\u001b[0m, in \u001b[0;36m_raw_face_locations\u001b[1;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cnn_face_detector(img, number_of_times_to_upsample)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mface_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_times_to_upsample\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from deepface import DeepFace\n",
    "import mediapipe as mp\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "\n",
    "    # Percorrer todos os arquivos na pasta fornecida\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\") or filename.endswith(\".jpeg\"):\n",
    "            image_path = os.path.join(folder, filename)\n",
    "            image = face_recognition.load_image_file(image_path)\n",
    "            face_encodings = face_recognition.face_encodings(image)\n",
    "            if face_encodings:\n",
    "                face_encoding = face_encodings[0]\n",
    "                name = os.path.splitext(filename)[0][:-1]\n",
    "                known_face_encodings.append(face_encoding)\n",
    "                known_face_names.append(name)\n",
    "\n",
    "    return known_face_encodings, known_face_names\n",
    "\n",
    "def detect_faces_and_emotions(video_path, output_path, known_face_encodings, known_face_names):\n",
    "    # Inicializa o módulo de detecção de pose do MediaPipe\n",
    "    mp_pose = mp.solutions.pose\n",
    "    \n",
    "    # Cria um objeto 'Pose' para realizar a estimativa de pose nos frames\n",
    "    pose = mp_pose.Pose()\n",
    "    \n",
    "    # Utilitário para desenhar as marcações de pose nos frames\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    \n",
    "    # Abre o arquivo de vídeo especificado para processamento\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Retorna um erro de não for possível abrir o vídeo\n",
    "    if not cap.isOpened():\n",
    "        print(\"Erro ao abrir o vídeo.\")\n",
    "        return\n",
    "\n",
    "    # Obtém a largura dos frames do vídeo e converte para inteiro\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "    # Obtém a altura dos frames do vídeo e converte para inteiro\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Obtém a taxa de quadros por segundo (FPS) do vídeo\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Obtém o número total de frames no vídeo\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Define o codec de vídeo para o formato MP4\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "    # Cria um objeto VideoWriter para salvar o vídeo de saída com o codec, FPS e dimensões especificadas\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Variáveis para contar os movimentos e detecção de atividades\n",
    "    arm_up = False\n",
    "    arm_movements_count = 0\n",
    "    jumping = False\n",
    "    jumping_count = 0\n",
    "    sitting = False\n",
    "    sitting_count = 0\n",
    "    waving_hand = False\n",
    "    waving_hand_count = 0\n",
    "    clapping = False\n",
    "    clapping_count = 0\n",
    "\n",
    "    analyzed_frames = 0\n",
    "    anomalies_detected = 0\n",
    "\n",
    "\n",
    "    def is_arm_up(landmarks):\n",
    "        # Obtém a posição do olho esquerdo a partir dos pontos de referência\n",
    "        left_eye = landmarks[mp_pose.PoseLandmark.LEFT_EYE.value]\n",
    "\n",
    "        # Obtém a posição do olho direito a partir dos pontos de referência\n",
    "        right_eye = landmarks[mp_pose.PoseLandmark.RIGHT_EYE.value]\n",
    "\n",
    "        # Obtém a posição do punho esquerdo a partir dos pontos de referência\n",
    "        left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
    "\n",
    "        # Obtém a posição do punho direito a partir dos pontos de referência\n",
    "        right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
    "\n",
    "        # Verifica se o punho esquerdo está acima do olho esquerdo (eixo Y: valores menores estão mais acima)\n",
    "        left_arm_up = left_wrist.y < left_eye.y\n",
    "        \n",
    "        # Verifica se o punho direito está acima do olho direito\n",
    "        right_arm_up = right_wrist.y < right_eye.y\n",
    "\n",
    "        # Retorna True se qualquer um dos braços estiver levantado acima do nível dos olhos\n",
    "        return left_arm_up or right_arm_up\n",
    "\n",
    "\n",
    "\n",
    "    def is_jumping(landmarks):\n",
    "        # Obtém a posição do tornozelo esquerdo a partir dos pontos de referência\n",
    "        left_ankle = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value]\n",
    "\n",
    "        # Obtém a posição do tornozelo direito a partir dos pontos de referência\n",
    "        right_ankle = landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value]\n",
    "\n",
    "        # Verifica se ambos os tornozelos são visíveis (visibility > 0.5) e estão acima de um certo nível no eixo Y (y < 0.5),\n",
    "        # o que indica que a pessoa está no ar, possivelmente pulando\n",
    "        return left_ankle.visibility > 0.5 and right_ankle.visibility > 0.5 and left_ankle.y < 0.5 and right_ankle.y < 0.5\n",
    "\n",
    "\n",
    "    def is_sitting(landmarks):\n",
    "        # Obtém a posição do quadril esquerdo a partir dos pontos de referência\n",
    "        left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "        \n",
    "        # Obtém a posição do quadril direito a partir dos pontos de referência\n",
    "        right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
    "\n",
    "        # Obtém a posição do joelho esquerdo a partir dos pontos de referência\n",
    "        left_knee = landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value]\n",
    "\n",
    "        # Obtém a posição do joelho direito a partir dos pontos de referência\n",
    "        right_knee = landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value]\n",
    "\n",
    "         # Verifica se ambos os quadris estão acima dos joelhos no eixo Y (valores menores estão mais acima),\n",
    "        # o que indica que a pessoa está sentada\n",
    "        return left_hip.y < left_knee.y and right_hip.y < right_knee.y\n",
    "\n",
    "\n",
    "    def is_waving_hand(landmarks):\n",
    "        # Obtém a posição do punho direito a partir dos pontos de referência\n",
    "        right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
    "\n",
    "        # Obtém a posição do cotovelo direito a partir dos pontos de referência\n",
    "        right_elbow = landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value]\n",
    "\n",
    "        # Verifica se o punho direito está acima do cotovelo direito no eixo Y (valores menores estão mais acima)\n",
    "        # e se o punho direito é visível (visibility > 0.5), indicando que a pessoa está acenando com a mão\n",
    "        return right_wrist.y < right_elbow.y and right_wrist.visibility > 0.5\n",
    "\n",
    "    def is_clapping(landmarks):\n",
    "        # Obtém a posição do punho esquerdo a partir dos pontos de referência\n",
    "        left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
    "\n",
    "        # Obtém a posição do punho direito a partir dos pontos de referência\n",
    "        right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
    "\n",
    "        # Calcula a distância euclidiana entre os punhos esquerdo e direito\n",
    "        distance = np.sqrt((left_wrist.x - right_wrist.x) ** 2 + (left_wrist.y - right_wrist.y) ** 2)\n",
    "        \n",
    "        # Verifica se a distância é menor que um limiar (0.05), indicando que as mãos estão próximas (palmas)\n",
    "        return distance < 0.05\n",
    "\n",
    "\n",
    "    for _ in tqdm(range(total_frames), desc=\"Processando vídeo\"):\n",
    "        # Lê o próximo frame do vídeo\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Se não conseguiu ler o frame (fim do vídeo), interrompe o loop\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Incrementa o contador de frames analisados\n",
    "        analyzed_frames += 1\n",
    "\n",
    "        # Analisa o frame usando DeepFace para extrair emoções, idade e raça\n",
    "        result = DeepFace.analyze(frame, actions=['emotion', 'age', 'race'], enforce_detection=False)\n",
    "        \n",
    "        # Converte o frame de BGR (padrão OpenCV) para RGB (padrão usado pelo face_recognition e MediaPipe)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detecta as localizações das faces no frame\n",
    "        face_locations = face_recognition.face_locations(rgb_frame)\n",
    "\n",
    "        # Obtém as codificações faciais para cada face detectada\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "        # Processa o frame com MediaPipe para obter os pontos de referência (landmarks) da pose corporal\n",
    "        results = pose.process(rgb_frame)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            # Se landmarks (pontos de referência da pose) foram detectados no frame atual\n",
    "            # Desenha as marcações da pose no frame usando as utilidades de desenho do MediaPipe\n",
    "            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "            # Obtém a lista de pontos de referência (landmarks) da pose detectada\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Arm Up Detection\n",
    "            if is_arm_up(landmarks):\n",
    "                if not arm_up:\n",
    "                    arm_up = True\n",
    "                    arm_movements_count += 1\n",
    "            else:\n",
    "                arm_up = False\n",
    "\n",
    "            # Jumping Detection\n",
    "            if is_jumping(landmarks):\n",
    "                if not jumping:\n",
    "                    jumping = True\n",
    "                    jumping_count += 1\n",
    "            else:\n",
    "                jumping = False\n",
    "            \n",
    "            \n",
    "            # Sitting Down Detection\n",
    "            if is_sitting(landmarks):\n",
    "                if not sitting:\n",
    "                    sitting = True\n",
    "                    sitting_count += 1\n",
    "            else:\n",
    "                sitting = False\n",
    "\n",
    "            # Waving Hand Detection\n",
    "            if is_waving_hand(landmarks):\n",
    "                if not waving_hand:\n",
    "                    waving_hand = True\n",
    "                    waving_hand_count += 1\n",
    "            else:\n",
    "                waving_hand = False\n",
    "\n",
    "            # Clapping Detection\n",
    "            if is_clapping(landmarks):\n",
    "                if not clapping:\n",
    "                    clapping = True\n",
    "                    clapping_count += 1\n",
    "            else:\n",
    "                clapping = False\n",
    "\n",
    "            # Escreve no frame a contagem de movimentos\n",
    "\n",
    "            cv2.putText(frame, f'Movimentos dos pulsos: {arm_movements_count}', (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.putText(frame, f'Contagem de palmas: {clapping_count}', (10, 150),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "            \n",
    "\n",
    "            cv2.putText(frame, f'Deteccao de Pulos: {jumping_count}', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.putText(frame, f'Deteccao de Sentadas: {sitting_count}', (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.putText(frame, f'Acenando com as maos: {waving_hand_count}', (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        else:\n",
    "             # Se não foram detectados pontos de referência da pose, incrementa o contador de anomalias\n",
    "            anomalies_detected += 1\n",
    "\n",
    "\n",
    "        face_names = []\n",
    "        for face_encoding in face_encodings:\n",
    "            # Compara a codificação facial atual com as codificações conhecidas para verificar possíveis correspondências\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "            \n",
    "            # Define o nome padrão como 'Desconhecido' caso não haja correspondência\n",
    "            name = \"Desconhecido\"\n",
    "\n",
    "            # Calcula a distância de face entre a codificação atual e todas as codificações conhecidas\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "\n",
    "            # Encontra o índice da menor distância (melhor correspondência)\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            # Se a correspondência for verdadeira no melhor índice, obtém o nome correspondente\n",
    "            if matches[best_match_index]:\n",
    "                name = known_face_names[best_match_index]\n",
    "            # Adiciona o nome (ou 'Desconhecido') à lista de nomes de faces detectadas\n",
    "            face_names.append(name)\n",
    "\n",
    "\n",
    "        for face in result:\n",
    "            # Extrai as coordenadas da região da face detectada pelo DeepFace\n",
    "            x, y, w, h = face['region']['x'], face['region']['y'], face['region']['w'], face['region']['h']\n",
    "            dominant_emotion = face['dominant_emotion']\n",
    "            age = face['age']\n",
    "            dominant_race = face['dominant_race']\n",
    "\n",
    "            # Cria uma lista de atributos para exibir no frame\n",
    "            attributes = [\n",
    "                f\"Emotion: {dominant_emotion}\",\n",
    "                f\"Age: {age} years\",\n",
    "                f\"Race: {dominant_race}\"\n",
    "            ]\n",
    "            # Calcula a posição vertical para cada atributo (empilhando-os abaixo da face)\n",
    "            for i, attr in enumerate(attributes):\n",
    "                text_y = y + h + 20 + (i * 25)\n",
    "                # Escreve cada atributo no frame, posicionando-os abaixo da região da face\n",
    "                cv2.putText(frame, attr, (x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "             # Associa os nomes às faces correspondentes\n",
    "            for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "                # Verifica se as coordenadas da face reconhecida coincidem com a região atual\n",
    "                if x <= left <= x + w and y <= top <= y + h:\n",
    "                    # Escreve o nome da pessoa acima da região da face\n",
    "                    cv2.putText(frame, name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 0), 2)\n",
    "                    # Interrompe o loop após encontrar a correspondência\n",
    "                    break\n",
    "                \n",
    "        # Escreve o frame processado no vídeo de saída\n",
    "        out.write(frame)\n",
    "    \n",
    "    # Libera os recursos de captura e escrita de vídeo e fecha todas as janelas OpenCV\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    print(\"\\nRelatório Resumido\")\n",
    "    print(\"------------------\")\n",
    "    print(f\"Total de Frames Analisados: {analyzed_frames}\")\n",
    "    print(f\"Total de Anomalias Detectadas: {anomalies_detected}\")\n",
    "\n",
    "image_folder = 'C:/Users/gabri/Desktop/FIAP/fiap_pos_tech/ImagesChallenge'\n",
    "known_face_encodings, known_face_names = load_images_from_folder(image_folder)\n",
    "input_video_path = 'C:/Users/gabri/Desktop/FIAP/fiap_pos_tech/VideosChallenge/videoTechChallenge.mp4'\n",
    "output_video_path = 'C:/Users/gabri/Desktop/FIAP/fiap_pos_tech/VideosChallenge/output3_video_tech_challenge.mp4'\n",
    "\n",
    "detect_faces_and_emotions(input_video_path, output_video_path, known_face_encodings, known_face_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiap_pos_tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
